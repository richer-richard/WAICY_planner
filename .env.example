# LLM Provider (server-side)
# Supported: deepseek | openai | gemini
LLM_PROVIDER=deepseek

# -------------------- DeepSeek --------------------
# Get your API key from https://platform.deepseek.com/
DEEPSEEK_API_KEY=your_deepseek_api_key_here
# Optional: DeepSeek model (default: deepseek-chat)
# DEEPSEEK_MODEL=deepseek-chat
# Optional: Custom DeepSeek base URL (OpenAI-compatible chat completions endpoint)
# DEEPSEEK_BASE_URL=https://api.deepseek.com/v1/chat/completions

# -------------------- OpenAI (GPT) --------------------
# Get your API key from https://platform.openai.com/
# OPENAI_API_KEY=your_openai_api_key_here
# Optional: OpenAI model (default: gpt-4o-mini)
# OPENAI_MODEL=gpt-4o-mini
# Optional: OpenAI-compatible chat completions endpoint
# OPENAI_BASE_URL=https://api.openai.com/v1/chat/completions

# -------------------- Google Gemini --------------------
# Get your API key from https://aistudio.google.com/app/apikey
# GEMINI_API_KEY=your_gemini_api_key_here
# Optional: Gemini model (default: gemini-1.5-flash)
# GEMINI_MODEL=gemini-1.5-flash
# Optional: Gemini API base URL (default: https://generativelanguage.googleapis.com/v1beta)
# GEMINI_BASE_URL=https://generativelanguage.googleapis.com/v1beta

# -------------------- MCP Server (optional) --------------------
# `npm run mcp` starts an MCP server over stdio for external agent clients.
# AXIS_API_BASE_URL=http://localhost:3000
# AXIS_API_TOKEN=paste-a-jwt-from-/api/auth/login

# -------------------- Auth (REQUIRED for login/register) --------------------
# Use a long random value (>=32 chars). Example generation:
#   node -e "console.log(require('crypto').randomBytes(32).toString('hex'))"
JWT_SECRET=change-me-to-a-long-random-string

# Optional: Server port (default: 3000)
# PORT=3000
